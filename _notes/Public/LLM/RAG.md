---
title : RAG (Retrieval-Augmented Generation)
notetype : feed
date : 08-02-2025
---

## From GPT-4o 

RAG หรือ Retrieval-Augmented Generation เป็นเทคนิคที่ใช้กับ Large Language Models (LLMs) เพื่อปรับปรุงคุณภาพของคำตอบโดยการผสมผสานการดึงข้อมูล (retrieval) และการสร้างข้อความ (generation) เข้าด้วยกัน

### หลักการทำงานของ RAG
โดยทั่วไป LLM มักอาศัยข้อมูลที่ถูกฝึกมาเพื่อตอบคำถาม แต่ข้อจำกัดคือข้อมูลเหล่านั้นอาจล้าสมัยหรือไม่ครอบคลุมทั้งหมด RAG ช่วยแก้ปัญหานี้โดยเพิ่มกระบวนการค้นหาข้อมูลแบบเรียลไทม์จากแหล่งภายนอก ก่อนนำมาใช้ในการสร้างคำตอบ
1. Retrieval (การดึงข้อมูล)
    - เมื่อมีคำถามเข้ามา ระบบจะไปดึงข้อมูลที่เกี่ยวข้องจาก แหล่งข้อมูลภายนอก เช่น เอกสาร, ฐานข้อมูล, หรือ vector database (เช่น FAISS, Pinecone)
2. Augmentation (การเสริมข้อมูล)
    - ข้อมูลที่ค้นมาได้จะถูกผนวกเข้ากับ prompt ที่ส่งเข้าไปยัง LLM เพื่อช่วยให้โมเดลสามารถใช้ข้อมูลเหล่านั้นในการสร้างคำตอบที่แม่นยำขึ้น
3. Generation (การสร้างคำตอบ)
    - LLM จะใช้ข้อมูลที่ได้รับการเสริมเติมเข้าไปเพื่อสร้างข้อความตอบกลับที่สอดคล้องและถูกต้องมากขึ้น

### ข้อดีของ RAG
- ✅ ลดปัญหาความผิดพลาด (Hallucination) – LLM จะมีแนวโน้มให้ข้อมูลที่ถูกต้องมากขึ้น เพราะอ้างอิงจากข้อมูลภายนอกแทนที่จะ "เดา"
- ✅ อัปเดตข้อมูลแบบเรียลไทม์ – ไม่ต้อง retrain โมเดลใหม่ เพียงแค่เปลี่ยนแหล่งข้อมูลที่ใช้ดึงมา
- ✅ ปรับแต่งเฉพาะทางได้ง่าย – เหมาะกับงานที่ต้องการความแม่นยำสูง เช่น AI สำหรับวิจัย, กฎหมาย, การแพทย์

### ตัวอย่างการใช้งาน RAG
- Chatbot ที่เข้าถึงฐานข้อมูลบริษัท เพื่อให้คำตอบที่อัปเดตเสมอ
- ระบบค้นหาข้อมูลเอกสารภายในองค์กร เช่น การค้นหากฎระเบียบหรือคู่มือใช้งาน
- AI ช่วยสรุปข่าวสารหรือข้อมูลวิชาการ โดยดึงข้อมูลจากแหล่งที่เชื่อถือได้

### เครื่องมือยอดนิยมที่ใช้ทำ RAG
- Vector Databases: FAISS, Pinecone, Weaviate, ChromaDB
- LLM Frameworks: LangChain, LlamaIndex
- Embeddings Models: OpenAI Embeddings, BERT, SentenceTransformers

![RAG LLM Integration](/assets/img/Other/LLM/rag_llm_integration.avif) 

## Useful link
- https://humanloop.com/blog/optimizing-llms