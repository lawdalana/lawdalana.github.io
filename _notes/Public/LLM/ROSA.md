---
title : ROSA
notetype : unfeed
date : 02-08-2025
---

# Random Subspace Adaptation for Efficient Fine-Tuning (ROSA)

## 1. ROSA คืออะไร

ROSA (Random Subspace Adaptation) คือวิธีการปรับจูนโมเดลขนาดใหญ่ (Large Language Models หรือ PLMs) ให้เข้ากับงานเฉพาะทาง (downstream tasks) ได้อย่างมีประสิทธิภาพในด้านพารามิเตอร์ (Parameter-Efficient Fine-Tuning หรือ PEFT)
วิธีการนี้ได้รับการออกแบบมาเพื่อลดข้อจำกัดด้านหน่วยความจำที่มักพบในการฝึกโมเดลขนาดใหญ่ โดยเฉพาะอย่างยิ่งเมื่อการฝึกแบบเต็มรูปแบบ (full fine-tuning) มีค่าใช้จ่ายสูงมาก ROSA มีเป้าหมายที่จะขยายความสามารถในการแสดงออก (expressivity) ของโมเดลที่ถูกปรับจูนให้ดียิ่งขึ้น ในขณะที่ยังคงประสิทธิภาพด้านหน่วยความจำเทียบเท่ากับ LoRA และไม่เพิ่มความหน่วงในการอนุมาน (inference latency)

## 2. ต่างจาก LoRA อย่างไร

ROSA แตกต่างจาก LoRA อย่างมีนัยสำคัญในหลายประเด็น:

* **ข้อจำกัดในการแสดงออก (Expressivity Limitation)**

  * LoRA จำกัดการอัปเดตน้ำหนักของโมเดลให้อยู่ในรูปแบบของเมทริกซ์ที่มีอันดับต่ำ (low-rank matrix) ซึ่งก่อให้เกิดอคติและลดความสามารถของโมเดลในการปรับจูนไปสู่เป้าหมายที่ต้องการได้อย่างสมบูรณ์
  * ในทางตรงกันข้าม ROSA ขจัดข้อจำกัดอันดับต่ำนี้และสามารถปรับจูนซับสเปซที่มีขนาดใหญ่ขึ้นได้อย่างต่อเนื่อง ทำให้มีความสามารถในการแสดงออกมากกว่า LoRA และใกล้เคียงกับการปรับจูนแบบเต็มรูปแบบได้ดีกว่า

* **การแซมพลิงซับสเปซและการรวมข้อมูล (Subspace Sampling and Merging)**

  * LoRA จะเพิ่มเมทริกซ์อันดับต่ำที่ฝึกได้แบบตายตัวคู่ขนานไปกับน้ำหนักโมเดลเดิมที่ตรึงไว้
  * ROSA จะสุ่มแซมพลิงซับสเปซของน้ำหนักใหม่ๆ ตลอดกระบวนการฝึกอบรม และรวมข้อมูลที่เรียนรู้เข้ากับน้ำหนักเดิมซ้ำๆ ช่วยขยายมิติซับสเปซได้อย่างมีประสิทธิภาพ

* **การเริ่มต้นค่า (Initialization)**

  * LoRA เริ่มต้นค่าอะแดปเตอร์ AB เป็นศูนย์ (AB = 0) ซึ่งเหมือนการเรียนรู้การแสดงผลใหม่โดยสิ้นเชิง
  * ROSA เริ่มต้นซับสเปซที่ฝึกได้โดยใช้ข้อมูลจากเมทริกซ์น้ำหนักที่ผ่านการฝึกไว้ล่วงหน้า (pre-trained weight matrices) ผ่านการใช้ SVD (Singular Value Decomposition)

* **การแลกเปลี่ยน (Trade-off)**

  * LoRA แลกเปลี่ยนความสามารถในการแสดงออกกับข้อกำหนดหน่วยความจำที่ต่ำ
  * ROSA แลกเปลี่ยนความเร็วในการบรรจบกัน (convergence speed) กับการใช้หน่วยความจำ — ยิ่งอันดับสูงขึ้นก็ยิ่งบรรจบเร็วขึ้น แต่แม้ในอันดับต่ำก็ยังบรรจบไปสู่โซลูชันที่ดีได้

## 3. ใช้เทคนิคอะไร

ROSA ใช้เทคนิคหลักสามอย่างที่ทำงานร่วมกัน:

1. **การเริ่มต้นค่าด้วย SVD (SVD Initialization)**

   * อะแดปเตอร์ของ ROSA จะถูกเริ่มต้นโดยใช้การสลายตัวค่าเอกฐาน (Singular Value Decomposition) ของเมทริกซ์น้ำหนักที่ผ่านการฝึกไว้ล่วงหน้า (W)
   * เลือกคอลัมน์และค่าเอกฐานจาก SVD ของ W เพื่อสร้างเมทริกซ์ A และ B ทำให้สามารถใช้ประโยชน์จากคุณสมบัติที่โมเดลได้เรียนรู้ไว้

2. **การแยกส่วน (Factorization)**

   * แยกเมทริกซ์น้ำหนัก W ออกเป็นส่วนที่ฝึกได้ (AB) และส่วนที่คงที่ (W<sub>fixed</sub>) โดย W = W<sub>fixed</sub> + AB
   * ในการฝึกอบรม คำนวณและปรับปรุงเฉพาะส่วน A และ B ซึ่งเป็นซับสเปซมิติ R ลดจำนวนพารามิเตอร์ที่ต้องฝึกจาก MN เหลือ R(M+N)

3. **การสุ่มแซมพลิงใหม่ (Resampling)**

   * ทุกๆ epoch ROSA จะรวมอะแดปเตอร์ที่เรียนรู้เข้ากับน้ำหนักเดิม (W ← W + AB)
   * ทำ SVD กับ W ที่อัปเดต (U, Σ, V<sup>⊤</sup> ← SVD(W)) แล้วสุ่มเลือกซับเซ็ต R ของเวกเตอร์เอกฐาน มาเริ่มต้นใหม่ให้ A และ B
   * กระบวนการนี้ช่วยขยายมิติซับสเปซของ W อย่างต่อเนื่อง ไม่ถูกจำกัดด้วยอคติอันดับต่ำแบบ LoRA

## 4. ผลลัพธ์มีอะไรดีกว่าเดิมบ้าง

ROSA แสดงประสิทธิภาพที่ดีขึ้นอย่างมีนัยสำคัญเมื่อเทียบกับ PEFT อื่นๆ เช่น LoRA และ (IA)<sup>3</sup>:

* **ประสิทธิภาพเทียบเท่า Full Fine-tuning**

  * ทฤษฎีว่า ROSA สามารถบรรจบไปสู่โซลูชันเดียวกับ full fine-tuning
  * ทดสอบบนข้อมูลสังเคราะห์ พบว่าให้ประสิทธิภาพใกล้เคียงกัน แม้กับโมเดลที่ไม่เป็นเชิงเส้นฝึกด้วย SGD
* **Natural Language Understanding (NLU)**

  * บน GLUE benchmark ด้วย RoBERTa<sub>base</sub> (125M พารามิเตอร์)
  * ตัวอย่าง: งาน CoLA ด้วย rank = 8, ROSA ได้ Matthew’s correlation = 64.80 เทียบกับ LoRA = 54.27 และ (IA)<sup>3</sup> = 55.18
* **Natural Language Generation (NLG)**

  * บน E2E NLG task ด้วย GPT-2
  * ตัวอย่าง: rank = 4, ROSA ได้ BLEU = 68 (เทียบเท่า full fine-tuning) ขณะที่ LoRA = 64 และ (IA)<sup>3</sup> = 65
* **ประสิทธิภาพหน่วยความจำและเวลา**

  * หน่วยความจำเทียบเท่า LoRA
  * ขั้นตอน factorization เพิ่มเวลาเพียงเล็กน้อย (4.03 วินาทีต่อ epoch สำหรับ RoBERTa<sub>base</sub> จากทั้งหมด 153 วินาที)

## 5. ข้อดีข้อเสียเมื่อเทียบกับ LoRA

**ข้อดี:**

* ความสามารถในการแสดงออกเหนือกว่า ไม่ถูกจำกัดด้วยอคติอันดับต่ำ
* ผลลัพธ์ดีกว่าในงาน NLU และ NLG
* ไม่เพิ่มความหน่วงในการอนุมาน (เหมือน LoRA)
* ใช้ประโยชน์จากคุณสมบัติ pre-trained ผ่าน SVD initialization
* ควบคุม trade-off ระหว่าง convergence speed กับการใช้หน่วยความจำได้

**ข้อเสีย:**

* ต้องจัดเก็บโมเดลทั้งหมดหลังการปรับจูน (ต่างจากอะแดปเตอร์บางวิธีที่แชร์ได้ระหว่างหลายงาน)
* เหมาะกับงานเฉพาะทางเดียว (PEFT อื่นๆ อาจเหมาะกว่าเมื่อต้องปรับจูนหลายงานพร้อมกัน)

## 6. แนวทางพัฒนาต่อยอด

* **ปรับใช้กับ Convolution Operations:** ปัจจุบันจำกัดที่เลเยอร์เชิงเส้น (linear layers) ใน Transformer
* **ลดการใช้พื้นที่ดิสก์สำหรับหลายงาน:** หาวิธีที่ ROSA ลดขนาดโมเดลลงเมื่อใช้กับหลาย downstream tasks
