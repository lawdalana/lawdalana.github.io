---
title : QLoRA
notetype : feed
date : 02-08-2025
---

## Paper: [](https://arxiv.org/pdf/2305.14314)

- การปรับแต่งโมเดลขนาดใหญ่มากๆ นั้น มีค่าใช้จ่ายที่สูงมาก โดยเฉพาะอย่างยิ่งในด้านหน่วยความจำ GPU
- ปรับแต่งแบบ 16-bit ทั่วไปสำหรับโมเดล LLaMA 65B parameter ต้องใช้หน่วยความจำ GPU มากกว่า 780 GB
- Quantization ล่าสุดที่สามารถลดการใช้หน่วยความจำของ LLMs ได้ แต่เทคนิคเหล่านี้มักใช้ได้กับการ inference เท่านั้น และ จะล้มเหลวในระหว่างการฝึกฝน (training)

![Parameter-Efficient Fine-Tuning](/assets/img/Other/LLM/applsci-15-03087-g001.png)

## 3 Innovation of QLoRA:

**1. 4-bit NormalFloat (NF4)**

**2. Double Quantization (DQ)**

**3. Paged Optimizers**



### 1. 4-bit NormalFloat (NF4): เป็นประเภทข้อมูลใหม่ที่เหมาะสมที่สุดตามทฤษฎีข้อมูล (information theoretically optimal) สำหรับน้ำหนักที่กระจายตัวแบบปกติ
 NF4 ให้ผลลัพธ์เชิงประจักษ์ที่ดีกว่า 4-bit Integer และ 4-bit Float นอกจากนี้ยังมีความสามารถในการเป็นตัวแทนของศูนย์ (discrete zeropoint) ได้อย่างแม่นยำ ซึ่งเป็นคุณสมบัติที่สำคัญสำหรับการ Quantize ค่าที่เป็นศูนย์โดยไม่มีข้อผิดพลาด
   - 8 bit = 1 byte
   - FP16 = floating-point 16 bit

#### ขั้นตอนการแปลง FP16 ให้เป็น NF4
ขั้นตอนการแปลงค่าจาก 16-bit (หรือ BFloat16) เป็น NF4:
การแปลงค่าจากความแม่นยำสูง (เช่น 16-bit float หรือ BFloat16) ไปเป็น NF4 ไม่ใช่การปัดเศษ (rounding) โดยตรง แต่เป็นการแมปค่าหลังจากการปรับขนาดให้เป็นมาตรฐาน (normalization) ไปยังค่าที่กำหนดไว้ล่วงหน้าของ NF4

1. ค่ามาตรฐานของ NF4 (Predefined NF4 Values): NF4 ประกอบด้วย 16 ค่าที่ไม่ต่อเนื่องกัน ซึ่งถูกคำนวณมาจากการประมาณค่าควอนไทล์ (quantiles) ของการกระจายตัวแบบปกติมาตรฐาน (Standard Normal Distribution N(0, 1)) และถูกปรับขนาดให้อยู่ในช่วง [-1, 1]
. ค่าที่แน่นอนของ NF4 มีดังนี้
: [-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]

NF4 มี 16 ค่า (7 ลบ + 1 ศูนย์ + 8 บวก)

2. การปรับขนาดน้ำหนักโมเดล (Normalization): ก่อนที่จะแมปค่า น้ำหนักของโมเดลที่ต้องการควอนไทซ์ (ซึ่งอาจเป็น 16-bit float หรือ BFloat16) จะถูกนำมาปรับขนาดให้อยู่ในช่วง [-1, 1] โดยการหารค่าแต่ละค่าในบล็อกนั้นด้วยค่าสัมบูรณ์สูงสุด (absolute maximum) ของบล็อกน้ำหนักนั้น
 การทำเช่นนี้ช่วยให้มั่นใจว่าข้อมูลจะถูกกระจายตัวอย่างเหมาะสมใน bins การควอนไทซ์

ตัวอย่างการแปลงค่าจาก 16-bit float เป็น NF4:
สมมติว่าเรามีน้ำหนักของโมเดลที่เป็น 16-bit float ในบล็อกหนึ่ง (หรือเทนเซอร์) และต้องการแปลงค่า 0.58 ซึ่งเป็นส่วนหนึ่งของบล็อกนั้นให้เป็น NF4.
- กำหนดค่า 16-bit float ที่จะแปลง: สมมติ W_BF16 = 0.58
- หามูลค่าสูงสุดสัมบูรณ์ของบล็อกน้ำหนัก: สมมติว่าค่าสูงสุดสัมบูรณ์ (absmax) ของบล็อกน้ำหนักที่ W_BF16 อยู่คือ 0.8 (ค่านี้ได้มาจากการวิเคราะห์บล็อกน้ำหนักทั้งหมด ไม่ใช่แค่ค่าเดียว)
- ปรับขนาดค่า W_BF16: นำ W_BF16 มาปรับขนาดให้อยู่ในช่วง [-1, 1] โดยการหารด้วย absmax ของบล็อกนั้น: W_normalized = W_BF16 / absmax = 0.58 / 0.8 = 0.725
- การควอนไทซ์ (Mapping to NF4): หาค่าในรายการ NF4 ทั้ง 16 ค่าที่ ใกล้เคียงที่สุด กับ W_normalized (0.725) เมื่อพิจารณาจากรายการค่า NF4 ข้างต้น:
    - 0.5626170039176941
    - 0.7229568362236023 (ค่านี้อยู่ใกล้ 0.725 มาก)
    - 1.0
- ค่า NF4 ที่ใกล้เคียงที่สุดกับ 0.725 คือ 0.7229568362236023

ดังนั้น ค่า 0.58 (ในรูปแบบ 16-bit float) เมื่อผ่านกระบวนการควอนไทซ์เป็น NF4 จะถูกแทนด้วยค่า 0.7229568362236023 ซึ่งเป็นหนึ่งใน 16 ค่าที่กำหนดไว้ของ NF4
- กระบวนการนี้ช่วยลดการใช้หน่วยความจำได้อย่างมาก ในขณะที่ยังคงรักษาประสิทธิภาพของโมเดลไว้ได้ดี.

---

### 2. Double Quantization (DQ): เป็นวิธีการ Quantize ค่าคงที่ของการ Quantization เพื่อประหยัดหน่วยความจำเพิ่มเติม
 DQ สามารถลดหน่วยความจำเฉลี่ยได้ประมาณ 0.37 bits ต่อพารามิเตอร์ หรือประมาณ 3 GB สำหรับโมเดล 65B

```
น้ำหนัก W ─┐
           │  [NF4 4-บิต]          (บล็อก 64)
           ├─► q (4b)
           │   c₂ (FP32) ──┐
           │               ├─► q₂ (FP8)      (บล็อก 256)
           │               └─► c₁ (FP32)
           └─► จัดเก็บ {q, q₂, c₁}
```

### Result (Memory & Accuracy)
**Memory** การประหยัดหน่วยความจำ


    Memorybase​ = 4-bit weights 0.5​​ + DQ constants 0.127 ​​= **0.516 B/param**

    เทียบกับ 16-บิต (2.0 B/param) ⇒ เล็กลง ≈ 3.9×
    LLaMA-65B: 33 GB (4-bit+DQ) vs 130 GB (fp16)
    arXiv


**Accuracy**
```
| Benchmark           | 16-bit LoRA | 4-bit NF4 | 4-bit NF4 + DQ |
| ------------------- | ----------- | --------- | -------------- |
| MMLU 7-65B (5-shot) | 53.0        | 52.2      | **53.1**       |
```

### Step by Step


```
w = [-0.90, -0.25, 0.00, 0.35, 0.75, -0.55, 0.14, -0.02, …]  # 64 ค่า
c₂  = max(|w|) = 0.90
w_scaled = w / c₂          # ย่อให้อยู่ในช่วง [-1,1]

NF4 Quantization (รอบแรก)
| w\_scaled | ค่าที่ใกล้สุดใน NF4      | ดัชนี 4 บิต |
| --------: | ------------------: | ----------- |
|     −0.90 |             −1.0000 | 0000        |
|     −0.25 |             −0.2844 | 0100        |
|      0.00 |              0.0000 | 0111        |
|      0.35 |              0.3379 | 1011        |
|      0.75 |              0.7229 | 1110        |
|     −0.55 |             −0.5251 | 0010        |
|      0.14 |              0.1609 | 1001        |
|     −0.02 |              0.0000 | 0111        |
เก็บ ดัชนี 4 บิต × 64 = 256 บิต
เก็บ c₂ 32 บิต (FP32)
รวบ 4 บล็อกเป็นกลุ่ม 256 น้ำหนัก สมมติ c₂ ของ 4 บล็อกคือ [0.90, 0.95, 0.85, 1.10]


Double Quantization (DQ)
c₁ = max(c₂) = 1.10        # FP32 อีก 32 บิต
c₂_norm = c₂ / c₁          # = [0.818, 0.864, 0.773, 1.000]

ควอนไทซ์ c₂_norm เป็น FP8 (8 บิต) ทีละค่า ตัวอย่างแมปเป็นค่ารหัส [209, 221, 198, 255]

ดังนั้นข้อมูลที่เก็บต่อ 256 พารามิเตอร์
c2_norm FP8 (8 บิต × 4) = 32 บิต
c1 32 บิต
รวม 64 บิต → 64 บิต / 256 = 0.25 บิต/พารามิเตอร์
เมื่อบวกน้ำหนัก 4 บิต → (4 + 0.25) / 8 = 0.531 ไบต์/พารามิเตอร์
สูตรละเอียดของ QLoRA ใช้เซ็นเตอร์ค่า c₂ เพิ่ม **ทำให้ได้ 0.127 บิต/พารามิเตอร์**

ลองมาคำนวนเล่นกัน
ดังนั้นข้อมูลที่เก็บต่อ 512 พารามิเตอร์
c2_norm FP8 (8 บิต × 8) = 64 บิต
c1 32 บิต
รวม 96 บิต → 96 บิต / 512 = 0.1875 บิต/พารามิเตอร์
เมื่อบวกน้ำหนัก 4 บิต → (4 + 0.1875) / 8 = 0.523 ไบต์/พารามิเตอร์

ดังนั้นข้อมูลที่เก็บต่อ 1024 พารามิเตอร์
c2_norm FP8 (8 บิต × 16) = 128 บิต
c1 32 บิต
รวม 160 บิต → 160 บิต / 1024 = 0.15625 บิต/พารามิเตอร์

.
.
.

ดังนั้นข้อมูลที่เก็บต่อ 16384 พารามิเตอร์
c2_norm FP8 (8 บิต × 256) = 2048 บิต
c1 32 บิต
รวม 2080 บิต → 2080 บิต / 16384 = **0.126953 บิต/พารามิเตอร์**

```

---

### 3. Paged Optimizers: เพื่อแก้ไขปัญหาการใช้หน่วยความจำที่พุ่งสูงขึ้น (memory spikes) ระหว่างการ Fine-tuning โมเดลภาษาขนาดใหญ่
คือกลไกที่ใช้ประโยชน์จากฟีเจอร์ Unified Memory (หน่วยความจำรวม) ของ NVIDIA ซึ่งช่วยในการถ่ายโอนข้อมูลระหว่าง CPU และ GPU โดยอัตโนมัติแบบหน้าต่อหน้า (page-to-page transfers) เพื่อให้การประมวลผลบน GPU เป็นไปอย่างราบรื่นและไม่มีข้อผิดพลาด แม้ในสถานการณ์ที่ GPU อาจมีหน่วยความจำไม่เพียงพอในบางครั้ง การทำงานของมันคล้ายกับการจัดการหน่วยความจำเสมือน (memory paging) ระหว่าง RAM ของ CPU และดิสก์

การจัดการ Memory Spikes และ Out-of-Memory Errors: เมื่อ GPU เริ่มมีหน่วยความจำไม่พอ (run out-of-memory) Paged Optimizers จะทำการ "ย้าย" (evict) สถานะของ Optimizer เหล่านี้จากหน่วยความจำ GPU ไปยัง RAM ของ CPU โดยอัตโนมัติ เมื่อสถานะของ Optimizer จำเป็นต้องใช้ในการอัปเดตโมเดล (optimizer update step) Paged Optimizers ก็จะ "โหลดกลับ" (paged back) เข้าสู่หน่วยความจำ GPU อีกครั้ง

การจัดการหน่วยความจำแบบนี้ช่วย ป้องกันไม่ให้เกิดข้อผิดพลาดหน่วยความจำเต็ม (out-of-memory errors) ซึ่งเป็นอุปสรรคสำคัญในการ Fine-tuning โมเดลขนาดใหญ่บน GPU ตัวเดียว ด้วย Paged Optimizers ทำให้สามารถ Fine-tuning โมเดลที่มีพารามิเตอร์จำนวนมาก เช่น โมเดล 33B หรือ 65B พารามิเตอร์ บน GPU ตัวเดียวที่มีหน่วยความจำจำกัดได้ (เช่น GPU ขนาด 24GB หรือ 48GB) ซึ่งหากไม่มีเทคนิคนี้อาจไม่สามารถทำได้

![Parameter-Efficient Fine-Tuning](/assets/img/Other/LLM/qlora_002.jpg)

---

## Summary

![Parameter-Efficient Fine-Tuning](/assets/img/Other/LLM/qlora_001.jpg)

- QLORA เป็นแนวทางการปรับแต่งที่มีประสิทธิภาพที่ช่วย ลดการใช้หน่วยความจำลงอย่างมาก จนสามารถปรับแต่งโมเดลขนาด 65B parameter บน GPU ขนาด 48GB เพียงตัวเดียวได้ โดยยังคงรักษาประสิทธิภาพการทำงานของ 16-bit finetuning ได้อย่างสมบูรณ์
- QLORA จะทำการ Backpropagate (การย้อนกลับของเกรเดียนท์) ผ่านโมเดลภาษาที่ถูก Quantized แบบ 4-bit และถูกแช่แข็ง (frozen) เข้าไปใน Low Rank Adapters (LoRA)
- วิธีการนี้ใช้เทคนิคความแม่นยำสูงในการ Quantize โมเดลที่ถูก Pretrained เป็น 4-bit จากนั้นจึงเพิ่มชุดของน้ำหนัก LoRA ที่สามารถเรียนรู้ได้จำนวนเล็กน้อย
- QLORA ใช้ประเภทข้อมูลสำหรับการจัดเก็บที่มีความแม่นยำต่ำ (โดยปกติคือ 4-bit NormalFloat) และประเภทข้อมูลสำหรับการคำนวณที่มีความแม่นยำสูง (โดยปกติคือ BFloat16)
 ในทางปฏิบัติ เมื่อมีการใช้น้ำหนัก QLORA ระบบจะทำการ Dequantize tensor เป็น BFloat16 ก่อนทำการคูณเมทริกซ์ในแบบ 16-bit
- สำหรับการอัปเดตพารามิเตอร์นั้น จำเป็นต้องใช้เฉพาะ gradient ที่สัมพันธ์กับข้อผิดพลาดของน้ำหนักอะแดปเตอร์เท่านั้น ไม่จำเป็นต้องใช้สำหรับน้ำหนัก 4-bit
ผลลัพธ์และผลกระทบที่สำคัญ:
- ประสิทธิภาพเทียบเท่า 16-bit: QLORA เป็นครั้งแรกที่แสดงให้เห็นว่าสามารถปรับแต่งโมเดล 4-bit แบบ Quantized ได้โดยไม่มีประสิทธิภาพลดลง
 ผลการทดลองแสดงให้เห็นอย่างสม่ำเสมอว่า QLORA 4-bit ที่ใช้ประเภทข้อมูล NF4 สามารถเทียบเท่ากับประสิทธิภาพของ 16-bit full finetuning และ 16-bit LoRA finetuning ได้ ในเกณฑ์มาตรฐานทางวิชาการ
- ความสามารถในการเข้าถึง: QLORA ลดความต้องการหน่วยความจำเฉลี่ยในการปรับแต่งโมเดล 65B parameter จาก >780GB ลงเหลือ <48GB
 ซึ่งถือเป็นการ เปลี่ยนแปลงที่สำคัญในการเข้าถึงการปรับแต่ง LLM ทำให้โมเดลขนาดใหญ่ที่สุดที่เผยแพร่ต่อสาธารณะในปัจจุบันสามารถปรับแต่งได้บน GPU เพียงตัวเดียว
- โมเดล Guanaco: ตระกูลโมเดล Guanaco ที่ปรับแต่งด้วย QLORA มีประสิทธิภาพดีกว่าโมเดลที่เปิดเผยก่อนหน้านี้ทั้งหมดใน Vicuna benchmark
 โดยโมเดล Guanaco 65B ทำได้ 99.3% ของประสิทธิภาพของ ChatGPT และใช้เวลาปรับแต่งเพียง 24 ชั่วโมงบน GPU ตัวเดียว โมเดล Guanaco 7B ขนาดเล็กที่สุด ใช้หน่วยความจำเพียง 5 GB และยังทำคะแนนได้ดีกว่าโมเดล Alpaca 26 GB ถึงกว่า 20 เปอร์เซ็นต์
- การศึกษาเชิงลึก: ประสิทธิภาพของ QLORA ทำให้สามารถทำการศึกษาเชิงลึกเกี่ยวกับการปรับแต่งคำแนะนำและการทำงานของ Chatbot ในขนาดโมเดลที่ไม่สามารถทำได้ด้วยการปรับแต่งแบบปกติ เนื่องจากหน่วยความจำที่ต้องใช้มากเกินไป
 มีการฝึกฝนโมเดลมากกว่า 1,000 โมเดลเพื่อการวิเคราะห์นี้
- ข้อมูลคุณภาพสำคัญกว่าขนาด: การศึกษาพบว่าคุณภาพของข้อมูลมีความสำคัญมากกว่าขนาดของชุดข้อมูล ตัวอย่างเช่น ชุดข้อมูล 9k ตัวอย่าง (OASST1) มีประสิทธิภาพเหนือกว่าชุดข้อมูล 450k ตัวอย่าง (FLAN v2) ในด้านประสิทธิภาพของ Chatbot
- การประเมิน Chatbot: มีการวิเคราะห์ประสิทธิภาพ Chatbot อย่างละเอียดโดยใช้ทั้งผู้ประเมินจากมนุษย์และ GPT-4 โดยพบว่าการประเมินด้วย GPT-4 เป็นทางเลือกที่คุ้มค่าและสมเหตุสมผลแทนการประเมินด้วยมนุษย์
- การเปิดเผยข้อมูล: โค้ด โมเดลทั้งหมด รวมถึง CUDA kernels สำหรับการฝึกฝน 4-bit ได้ถูกเผยแพร่สู่สาธารณะแล้ว ซึ่งรวมถึงการรวมวิธีการเข้ากับ Hugging Face transformers stack
- ผลกระทบเชิงบวก: QLORA ถือเป็น ปัจจัยที่ทำให้เกิดความเท่าเทียมกัน ที่ช่วยลดช่องว่างด้านทรัพยากรระหว่างบริษัทขนาดใหญ่กับทีมขนาดเล็กที่มี GPU สำหรับผู้บริโภค
 นอกจากนี้ยังเปิดประตูสู่การนำ LLMs ไปปรับแต่งบนโทรศัพท์มือถือและการใช้งานที่คำนึงถึงความเป็นส่วนตัว


---

## 6. Empirical Results & Benchmarks (2 min)

- **Accuracy retention**  
  - On common NLP benchmarks (e.g., GLUE/SQuAD), QLoRA typically matches FP16+LoRA within ~±0.1–0.5% given sane ranks and targets—i.e., near-lossless for many tasks.

- **Throughput & GPU-RAM**  
  - ~4× lower GPU memory for base weights, enabling larger batch/sequence or bigger models per GPU.  
  - End-to-end training speedups of roughly 1.2×–1.5× on A100-class hardware, with variance by model size, sequence length, and kernel fusion.

- **การคงความแม่นยำ**  
  - บนเบนช์มาร์ก NLP ทั่วไป (เช่น GLUE/SQuAD) QLoRA มักทำได้ใกล้เคียง FP16+LoRA ภายใน ~±0.1–0.5% หากตั้งค่า rank/targets เหมาะสม — ใกล้ไร้ความสูญเสียสำหรับหลายงาน

- **ทรูกพุต & หน่วยความจำ GPU**  
  - หน่วยความจำ GPU สำหรับน้ำหนักฐานลดลง ~4× ช่วยให้ใช้แบตช์/ความยาวซีเควนซ์ใหญ่ขึ้นหรือโมเดลใหญ่ขึ้นต่อ GPU  
  - ความเร็วปลายทางต่อปลายทางเพิ่มราว 1.2×–1.5× บนฮาร์ดแวร์ระดับ A100 ทั้งนี้ขึ้นกับขนาดโมเดล ความยาวซีเควนซ์ และการฟิวส์เคอร์เนล



## Comparision table Lora & QLora

![LLM recommendation from google](/assets/img/Other/image.avif)


## *[Fine-Tuning Large Language Models with LORA Slide](https://docs.google.com/presentation/d/1yVhLz403YsCUI3C3mg59-a5OyT2z6hBqP5KcY7z1mJE/edit#slide=id.p)*
